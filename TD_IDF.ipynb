{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e890ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da92f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ileshyadav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ileshyadav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ileshyadav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11c6f8",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "This code loops from the folders of TXT and takes every .txt file that does not start with ._ which contain the speeches per country per year. These are then added to a dataframe, storing the year, country code and the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e038f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../TXT\")\n",
    "\n",
    "name_text = []\n",
    "for folder in path.iterdir():\n",
    "    if folder.is_dir():\n",
    "        files = [f for f in folder.glob(\"*.txt\") if not f.name.startswith(\"._\")]\n",
    "\n",
    "        for file in files:\n",
    "            name = file.name\n",
    "            text = file.read_text(encoding=\"utf-8\")\n",
    "            \n",
    "            name_text.append({\n",
    "                        \"country\": name[:3],\n",
    "                        \"year\": name[-8:-4],\n",
    "                        \"speech\": text\n",
    "                    })\n",
    "    \n",
    "df = pd.DataFrame(name_text)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee99a3",
   "metadata": {},
   "source": [
    "## Punctuation - Stopwords - Tokenizing\n",
    "The following code removes all punctuation from the texts, it also tokenizes the string (returns a list of each word separately as a string) and removes stopwords from it and non alphabetical tokens\n",
    "\n",
    "https://www.geeksforgeeks.org/nlp/removing-stop-words-nltk-python/ \n",
    "\n",
    "https://www.geeksforgeeks.org/python/python-remove-punctuation-from-string/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e120ece",
   "metadata": {},
   "source": [
    "We also remove words that are related to countries as can be found in the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04c21c",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv(\"List_of_adjectival_and_demonymic_forms_for_countries_and_nations_1.csv\")\n",
    "countries_flat = countries_df.values.ravel().tolist()\n",
    "countries = []\n",
    "\n",
    "def split_small_capital(text):\n",
    "    split = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text).split()\n",
    "    countries.extend(split)\n",
    "\n",
    "for country_adj in countries_flat: split_small_capital(country_adj)\n",
    "\n",
    "countries = [country.lower()for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda82dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df_tokenize = df.copy()\n",
    "\n",
    "def punc_stop_token(speech : str):\n",
    "    speech_no_punctuation = re.sub(r'[^\\w\\s]', '', speech)\n",
    "    tokens = word_tokenize(speech_no_punctuation.lower())\n",
    "    new_speech = [word for word in tokens if (word.isalpha()) and (word not in stop_words) and (word not in countries)]\n",
    "    return new_speech\n",
    "\n",
    "speeches = df_tokenize['speech']\n",
    "new_speeches = speeches.apply(punc_stop_token)\n",
    "df_tokenize['speech_token'] = new_speeches\n",
    "df_tokenize.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d29ddb-24f4-455b-b430-5da47f8ce698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bigram frequency check (after tokenization) ---\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "all_bigrams = []\n",
    "for tokens in df_tokenize['speech_token']:   # your tokens are in this column\n",
    "    if isinstance(tokens, list):\n",
    "        all_bigrams.extend(list(ngrams(tokens, 2)))\n",
    "\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "print(\"Top 20 most common bigrams:\")\n",
    "for (w1, w2), cnt in bigram_counts.most_common(20):\n",
    "    print(f\"{w1} {w2:<20} {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acee553",
   "metadata": {},
   "source": [
    "## Polarization score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a929742",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarization_df = pd.read_csv(\"political-polarization-score.csv\")\n",
    "polarization_df.head()\n",
    "\n",
    "polarization_df.columns = polarization_df.columns.str.lower()\n",
    "\n",
    "bounds = [ -3, -1, 1, 3]\n",
    "\n",
    "labels = [\n",
    "    \"Stable\",\n",
    "    \"Neutral\",\n",
    "    \"Polarized\"\n",
    "]\n",
    "\n",
    "polarization_df[\"polarization label\"] = pd.cut(\n",
    "    polarization_df[\"political polarization score (central estimate)\"],\n",
    "    bins=bounds,\n",
    "    labels=labels,\n",
    "    include_lowest=True,\n",
    "    right=False\n",
    ")\n",
    "polarization_df = polarization_df.rename(columns={\"code\": \"country\"})\n",
    "\n",
    "\n",
    "polarization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (df_tokenize, polarization_df):\n",
    "    df['country'] = df['country'].astype(str).str.strip()\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "\n",
    "bad_tf = df_tokenize[df_tokenize['year'].isna()]\n",
    "bad_pol = polarization_df[polarization_df['year'].isna()]\n",
    "\n",
    "merged_df = df_tokenize.merge(\n",
    "    polarization_df,\n",
    "    how='right',\n",
    "    on=['country', 'year']\n",
    ")\n",
    "merged_df = merged_df.drop(columns=['entity'])\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a45dd",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "The following code calculates the TF-IDF score for each word in every speech, this is then stored in the Dataframe as a list of pairs, containing (word, tf-idf score), sorted descendingly, so you get the higher TF-IDF scores first\n",
    "\n",
    "https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0feb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = merged_df.copy()\n",
    "\n",
    "df_tf_idf['speech_token'] = df_tf_idf['speech_token'].str.join(' ')\n",
    "\n",
    "tfidf_vector = TfidfVectorizer()\n",
    "speeches = df_tf_idf['speech_token']\n",
    "\n",
    "tf_idf_matrix  = tfidf_vector.fit_transform(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1af3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33594f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vector.get_feature_names_out()\n",
    "\n",
    "def matrix_to_tfidf_pairs(row):\n",
    "    row_array = row.toarray().flatten()  \n",
    "    word_tf_idf_pairs = [(word, score) for word, score in zip(feature_names, row_array) if score > 0]\n",
    "    pairs_sorted = sorted(word_tf_idf_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return pairs_sorted\n",
    "\n",
    "df_tf_idf['speech_score'] = [matrix_to_tfidf_pairs(tf_idf_matrix[i]) for i in range(tf_idf_matrix.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1aace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf[['country', 'year', 'speech_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8054ab9-7d11-49ff-afce-20f47a374d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TF-IDF with unigrams + bigrams (n-grams) ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# use the same joined text you created above\n",
    "texts = df_tf_idf['speech']  # already ' '.join(tokens)\n",
    "\n",
    "tfidf_ngram = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # keep words and numbers\n",
    "    ngram_range=(1, 2),            # unigrams + bigrams\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    ")\n",
    "\n",
    "tf_idf_matrix_ngram = tfidf_ngram.fit_transform(texts)\n",
    "feature_names_ngram = tfidf_ngram.get_feature_names_out()\n",
    "\n",
    "print(\"Docs:\", tf_idf_matrix_ngram.shape[0],\n",
    "      \"Vocab size (with bigrams):\", tf_idf_matrix_ngram.shape[1])\n",
    "\n",
    "def matrix_to_tfidf_pairs_ng(row):\n",
    "    arr = row.toarray().flatten()\n",
    "    pairs = [(w, s) for w, s in zip(feature_names_ngram, arr) if s > 0]\n",
    "    return sorted(pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# store bigram-aware scores in a NEW column, while keeping original intact\n",
    "df_tf_idf['speech_score_bigrams'] = [\n",
    "    matrix_to_tfidf_pairs_ng(tf_idf_matrix_ngram[i])\n",
    "    for i in range(tf_idf_matrix_ngram.shape[0])\n",
    "]\n",
    "\n",
    "# quick peek into the dataframe with speech score for both unigrams AND bigrams\n",
    "df_tf_idf[['country', 'year', 'speech_score_bigrams']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7822c2",
   "metadata": {},
   "source": [
    "## Linear Regression - Lasso\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cde2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lin = merged_df.copy()\n",
    "df_lin = df_lin.rename(columns={\"political polarization score (central estimate)\": \"polarization score\"})\n",
    "\n",
    "# y = np.array(df_lin[\"polarization score\"]).reshape(-1, 1)\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "# df_lin[\"polarization scaled\"] = y_scaled\n",
    "\n",
    "\n",
    "X = tf_idf_matrix\n",
    "# y = df_lin['polarization scaled']\n",
    "y = df_lin['polarization score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_lin = linear_model.Ridge(alpha=0.1)\n",
    "model_lin.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_lin.predict(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", MSE)\n",
    "print(\"Mean Absolute Error:\", MAE)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71bdf8",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ee89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf_idf_matrix\n",
    "y = merged_df['polarization label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
